{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8207122a",
   "metadata": {},
   "source": [
    "#  Model Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "865f44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.testing import assert_close\n",
    "torch.manual_seed(42)\n",
    "from src import tool\n",
    "from tqdm import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e16e88",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IS_SKIP_TEST =True\n",
    "PAD_ID =0\n",
    "TEST_CONFIG = {\n",
    "    \"num_epochs\":10,\n",
    "    \"batch_size\":4,\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_len\": 256,  # 上下文长度\n",
    "    \"emb_dim\": 512,          # 嵌入维度\n",
    "    \"n_heads\": 8,           # 注意力头的数量\n",
    "    \"n_layers\": 12,          # 层数\n",
    "    \"drop_rate\": 0.1,        # dropout率\n",
    "    \"initializer_range\":0.02,\n",
    "    \"qkv_bias\": False ,      # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "TOKEN_TYPE=\"gpt2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e2af6",
   "metadata": {},
   "source": [
    "### Set device to (type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cb94447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbffcf",
   "metadata": {},
   "source": [
    "## Define Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df7316da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, norm_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "        \n",
    "\n",
    "class DummyGPT(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim']) #  “字典表”  (vocab_size, emb_dim) vocab_size 行，每一行对应一个 token 的emb_dim维的向量 \n",
    "        self.pos_emb = nn.Embedding(cfg['context_len'],cfg['emb_dim']) # (context_len, emb_dim)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks =  nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'],cfg['vocab_size'],bias=False \n",
    "        )# y = x · W^T + b   W的形状为[vocab_size,emb_dim] 本质是计算x与W的相似度 ，得到vocab_size个y向量\n",
    "       \n",
    "    def forward(self,in_idx):\n",
    "        #in_idx 通常是一个整数张量（Tensor），形状一般为 (batch_size, seq_len)\n",
    "        batch_size, seq_len = in_idx.shape  #in_idx 每个元素都是 token 的索引（范围是 [0, vocab_size-1])\n",
    "        tok_embeds = self.tok_emb(in_idx) #查“字典表”映射  嵌入向量(batch_size, seq_len)-->(batch_size, seq_len, emb_dim) \n",
    "\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))  #生成一个从 0 到 seq_len-1 的整数序列 (seq_len,) -->(seq_len, emb_dim)\n",
    "   \n",
    "        x = tok_embeds + pos_embeds #pos_embeds会自动广播为 -->(batch_size, seq_len, emb_dim)\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x) #-->(batch_size, seq_len, emb_dim)\n",
    "        logits = self.out_head(x) #(batch_size, seq_len, emb_dim)-->(batch_size, seq_len, vocab_size)\n",
    "        return logits\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7584ac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyGPT(\n",
       "  (tok_emb): Embedding(50257, 512)\n",
       "  (pos_emb): Embedding(256, 512)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): DummyTransformerBlock()\n",
       "    (1): DummyTransformerBlock()\n",
       "    (2): DummyTransformerBlock()\n",
       "    (3): DummyTransformerBlock()\n",
       "    (4): DummyTransformerBlock()\n",
       "    (5): DummyTransformerBlock()\n",
       "    (6): DummyTransformerBlock()\n",
       "    (7): DummyTransformerBlock()\n",
       "    (8): DummyTransformerBlock()\n",
       "    (9): DummyTransformerBlock()\n",
       "    (10): DummyTransformerBlock()\n",
       "    (11): DummyTransformerBlock()\n",
       "  )\n",
       "  (final_norm): DummyLayerNorm()\n",
       "  (out_head): Linear(in_features=512, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tool.skip_execution(skip=IS_SKIP_TEST)\n",
    "def test_dummyModel():\n",
    "    model = DummyGPT(TEST_CONFIG)\n",
    "    return model\n",
    "\n",
    "test_dummyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7a9ed",
   "metadata": {},
   "source": [
    "## Define LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0220738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim =-1 ,keepdim =True, unbiased =False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c41b2",
   "metadata": {},
   "source": [
    "### test layerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22a357ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0008,  0.3639, -1.3647],\n",
      "         [-1.0695, -0.2666,  1.3361],\n",
      "         [ 0.7862, -1.4111,  0.6249],\n",
      "         [ 0.5489,  0.8543, -1.4032],\n",
      "         [-0.1638, -1.1346,  1.2984]],\n",
      "\n",
      "        [[ 0.6228, -1.4109,  0.7881],\n",
      "         [ 1.3713, -0.9851, -0.3862],\n",
      "         [ 1.3246, -1.0913, -0.2334],\n",
      "         [-0.6678,  1.4135, -0.7457],\n",
      "         [ 1.3589, -1.0186, -0.3403]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[ 1.0008,  0.3639, -1.3647],\n",
      "         [-1.0695, -0.2666,  1.3361],\n",
      "         [ 0.7862, -1.4111,  0.6249],\n",
      "         [ 0.5489,  0.8543, -1.4032],\n",
      "         [-0.1638, -1.1346,  1.2984]],\n",
      "\n",
      "        [[ 0.6228, -1.4109,  0.7881],\n",
      "         [ 1.3713, -0.9851, -0.3862],\n",
      "         [ 1.3246, -1.0913, -0.2334],\n",
      "         [-0.6678,  1.4135, -0.7457],\n",
      "         [ 1.3589, -1.0186, -0.3403]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "自定义LayerNorm与官方实现输出一致\n"
     ]
    }
   ],
   "source": [
    "@tool.skip_execution(skip=IS_SKIP_TEST)\n",
    "def test_layer_norm():\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    emb_dim = 3  \n",
    "    x = torch.randn(batch_size, seq_len, emb_dim)  # 随机生成输入张量\n",
    "    \n",
    "    custom_ln = LayerNorm(emb_dim)\n",
    "    official_ln = nn.LayerNorm(emb_dim, eps=1e-5, elementwise_affine=True)\n",
    "    \n",
    "\n",
    "    official_ln.weight.data.copy_(custom_ln.scale.data)\n",
    "    official_ln.bias.data.copy_(custom_ln.shift.data)\n",
    " \n",
    "    custom_out = custom_ln(x)\n",
    "    official_out = official_ln(x)\n",
    "    print(custom_out)\n",
    "    print(official_out)\n",
    "\n",
    "    assert_close(\n",
    "        custom_out, \n",
    "        official_out, \n",
    "        rtol=1e-5,  # 相对误差容忍度\n",
    "        atol=1e-5   # 绝对误差容忍度\n",
    "    )\n",
    "    print(\"自定义LayerNorm与官方实现输出一致\")\n",
    "\n",
    "test_layer_norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396726f8",
   "metadata": {},
   "source": [
    "## Define Activation Function\n",
    "\n",
    "高斯误差线性单元 GELU\n",
    "Φ(x) ≈ 0.5 * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8633271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return 0.5*x*(1+ \n",
    "                      torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))\n",
    "                                 *(x+0.044715*torch.pow(x,3))\n",
    "                                )\n",
    "                      )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf57d8",
   "metadata": {},
   "source": [
    "### test gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9027426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入值: tensor([-3.0000, -1.0000,  0.0000,  0.5000,  1.0000,  2.0000,  5.0000])\n",
      "自定义GELU输出: tensor([-3.6374e-03, -1.5881e-01,  0.0000e+00,  3.4571e-01,  8.4119e-01,\n",
      "         1.9546e+00,  5.0000e+00])\n",
      "官方GELU输出: tensor([-4.0499e-03, -1.5866e-01,  0.0000e+00,  3.4573e-01,  8.4134e-01,\n",
      "         1.9545e+00,  5.0000e+00])\n",
      "\n",
      "自定义GELU与官方实现近似一致\n"
     ]
    }
   ],
   "source": [
    "@tool.skip_execution(skip=IS_SKIP_TEST)\n",
    "def test_gelu():\n",
    "    x = torch.tensor([-3.0, -1.0, 0.0, 0.5, 1.0, 2.0, 5.0])\n",
    "    \n",
    "    custom_gelu = GELU()\n",
    "    official_gelu = nn.GELU()\n",
    "\n",
    "    custom_out = custom_gelu(x)\n",
    "    official_out = official_gelu(x)\n",
    "    \n",
    "    # 打印结果进行直观对比\n",
    "    print(\"输入值:\", x)\n",
    "    print(\"自定义GELU输出:\", custom_out)\n",
    "    print(\"官方GELU输出:\", official_out)\n",
    " \n",
    "    assert_close(\n",
    "        custom_out,\n",
    "        official_out,\n",
    "        rtol=1e-3,  # 相对误差容忍度\n",
    "        atol=1e-3   # 绝对误差容忍度\n",
    "    )\n",
    "    print(\"\\n自定义GELU与官方实现近似一致\")\n",
    "    \n",
    "test_gelu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b26a66",
   "metadata": {},
   "source": [
    "## Define FFN\n",
    "通过两层线性变换和激活函数，对注意力机制输出的特征进行非线性加工，增强模型表达能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63108816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    \n",
    "        #中间层hidden_dim通常设为4*emb_dim（如原始 Transformer 中为 512→2048→512），通过扩展维度捕捉更丰富的特征\n",
    "        self.c_fc=nn.Linear(cfg['emb_dim'],4*cfg['emb_dim'])\n",
    "        self.act=   GELU()\n",
    "        self.dropout=   nn.Dropout(cfg['drop_rate'])\n",
    "        self.c_proj=  nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
    "        self.c_proj.weight.data.normal_(\n",
    "            mean=0.0, \n",
    "            std=cfg['initializer_range'] / math.sqrt(2 * cfg['n_layers'])  # 层数相关的缩放\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6628b29",
   "metadata": {},
   "source": [
    "## Define MultiAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d5d1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_len,dropout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out =d_out\n",
    "        self.W_q = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #缓冲区（buffer）是模型中不需要被训练的参数（与 nn.Parameter 不同，后者是可学习参数），但会随模型一起保存（state_dict 中包含）\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(torch.ones(context_len,context_len),diagonal=1)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in = x.shape\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        att_score = queries @ keys.transpose(1,2)\n",
    "        att_score.masked_fill_(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf) # 上面的register_buffer  形状为 (num_tokens, num_tokens) 的子矩阵\n",
    "        att_weight = torch.softmax(att_score/keys.shape[-1]**0.5, dim=-1)\n",
    "        att_weight = self.dropout(att_weight)\n",
    "        context_vec = att_weight @ values\n",
    "        return context_vec\n",
    "        \n",
    "class MultiHeadAttendtion(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_len,dropout,num_heads,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # ModuleList与nn.Sequential不同，它不自动执行前向传播，而是需要手动遍历调用，适合需要单独处理每个子模块的场景\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in,d_out,context_len,dropout,qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7472d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 更高效的MutiAttention 减少计算量\n",
    "\n",
    "#参数规模更小（num_heads×d_model×d_model 对比 num_heads×d_model×head_dim)\n",
    "class MultiHeadAttendtion_new(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_len,dropout,num_heads,initializer_range,n_layer,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out =d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_q = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.c_proj =nn.Linear(d_out,d_out) # out_proj 可以学习如何 “融合” 这些头的信息（例如对不同头的特征赋予不同权重），而不是简单保留原始拼接结果\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(torch.ones(context_len,context_len),diagonal=1)\n",
    "        )\n",
    "        self.c_proj.weight.data.normal_(\n",
    "            mean=0.0, \n",
    "            std=initializer_range / math.sqrt(2 *n_layer)  # 层数相关的缩放\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in = x.shape\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        \n",
    "        #(b,num_tokens,num_heads,head_dim) --> (b,num_heads,num_tokens,head_dim)   \n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries .transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        \n",
    "        att_score = queries @ keys.transpose(2,3)\n",
    "        att_score.masked_fill_(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf)\n",
    "        att_weight = torch.softmax(att_score/keys.shape[-1]**0.5, dim=-1)\n",
    "        # att_weight = self.dropout(att_weight)\n",
    "        context_vec = (att_weight @ values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vec = self.c_proj(context_vec)\n",
    "        context_vec = self.dropout(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO With kV cache\n",
    "\n",
    "\n",
    "class MultiHeadAttendtion_KVCache(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_len,dropout,num_heads,initializer_range,n_layer,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out =d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.context_len =context_len\n",
    "        self.W_q = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in,d_out,bias= qkv_bias)\n",
    "        self.c_proj =nn.Linear(d_out,d_out) # out_proj 可以学习如何 “融合” 这些头的信息（例如对不同头的特征赋予不同权重），而不是简单保留原始拼接结果\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(torch.ones(context_len,context_len),diagonal=1)\n",
    "        )\n",
    "        self.c_proj.weight.data.normal_(\n",
    "            mean=0.0, \n",
    "            std=initializer_range / math.sqrt(2 *n_layer)  # 层数相关的缩放\n",
    "        )\n",
    "    \n",
    "    def forward(self,x,past_kv=None,use_cache=False,attention_mask=None):\n",
    "        b,new_seq_len,d_in = x.shape\n",
    "        \n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        keys = keys.view(b,new_seq_len,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b,new_seq_len,self.num_heads,self.head_dim)\n",
    "        values = values.view(b,new_seq_len,self.num_heads,self.head_dim)\n",
    "        #(b,num_tokens,num_heads,head_dim) --> (b,num_heads,num_tokens,head_dim)   \n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries .transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # 有效token的掩码（扩展维度适配KV形状）\n",
    "            valid_mask = attention_mask.unsqueeze(1).unsqueeze(-1)  # [b, 1, new_seq_len, 1]\n",
    "            valid_mask = valid_mask.expand(-1, self.num_heads, -1, self.head_dim)  # [b, num_heads, new_seq_len, head_dim]\n",
    "            # 过滤KV中的padding部分（将padding位置的KV置为0，后续不参与计算）\n",
    "            keys = keys * valid_mask\n",
    "            values = values * valid_mask\n",
    "        \n",
    "            valid_lens = attention_mask.sum(dim=1).long()  # [b]，每个样本的有效长度\n",
    "        else:\n",
    "            # 无padding时，有效长度等于序列长度\n",
    "            valid_lens = torch.full((b,), new_seq_len, dtype=torch.long, device=x.device)\n",
    "            \n",
    "        max_keep_len = self.context_len \n",
    "        # 处理KV缓存（首次缓存时跳过padding）\n",
    "        if past_kv is not None:\n",
    "            # 复用历史缓存：仅拼接有效部分（past_kv包含历史有效KV和长度）\n",
    "            past_keys, past_values, past_valid_lens = past_kv\n",
    "            # 计算新的总有效长度（历史有效长度 + 新有效长度）\n",
    "            total_valid_lens = past_valid_lens + valid_lens  # [b]\n",
    "\n",
    "            max_total_len = total_valid_lens.max()  # 批次内最大总有效长度（用于统一形状）\n",
    "            \n",
    "            # 初始化新的KV缓存（仅保留有效部分）\n",
    "            new_keys = torch.zeros(b, self.num_heads, max_total_len, self.head_dim, device=x.device)\n",
    "            new_values = torch.zeros_like(new_keys)\n",
    "            \n",
    "            for i in range(b):\n",
    "                # 拼接历史有效KV和新有效KV\n",
    "                past_len = past_valid_lens[i]  # 第i个样本的历史有效长度\n",
    "                curr_len = valid_lens[i]       # 第i个样本的新有效长度\n",
    "                # 复制历史有效部分\n",
    "                new_keys[i, :, :past_len, :] = past_keys[i, :, :past_len, :]\n",
    "                new_values[i, :, :past_len, :] = past_values[i, :, :past_len, :]\n",
    "                # 复制新有效部分（跳过padding）\n",
    "                new_keys[i, :, past_len:past_len+curr_len, :] = keys[i, :, :curr_len, :]\n",
    "                new_values[i, :, past_len:past_len+curr_len, :] = values[i, :, :curr_len, :]\n",
    "                \n",
    "            if max_total_len > max_keep_len:\n",
    "                # 直接截取最后max_keep_len长度的内容\n",
    "                new_keys = new_keys[:, :, -max_keep_len:, :]\n",
    "                new_values = new_values[:, :, -max_keep_len:, :]\n",
    "                # 更新有效长度（不超过max_keep_len）\n",
    "                total_valid_lens = torch.clamp(total_valid_lens, max=max_keep_len)\n",
    "                \n",
    "            keys = new_keys\n",
    "            values = new_values\n",
    "        else:\n",
    "            # 首次缓存：仅保留有效token的KV（跳过padding）\n",
    "            valid_lens = torch.minimum(valid_lens, torch.full_like(valid_lens, max_keep_len))\n",
    "            max_valid_len = valid_lens.max()  # 批次内最大有效长度\n",
    "            # 初始化缓存（仅分配有效长度的空间）\n",
    "            valid_keys = torch.zeros(b, self.num_heads, max_valid_len, self.head_dim, device=x.device)\n",
    "            valid_values = torch.zeros_like(valid_keys)\n",
    "            \n",
    "            for i in range(b):\n",
    "                # 仅存储有效token的KV（截断padding部分）\n",
    "                valid_len = valid_lens[i]\n",
    "                valid_keys[i, :, :valid_len, :] = keys[i, :, :valid_len, :]\n",
    "                valid_values[i, :, :valid_len, :] = values[i, :, :valid_len, :]\n",
    "            \n",
    "            keys = valid_keys\n",
    "            values = valid_values\n",
    "            total_valid_lens = valid_lens  # 首次缓存的总有效长度\n",
    "        \n",
    "        # 准备当前缓存（包含有效KV和有效长度，用于后续复用）\n",
    "        present_kv = (keys, values, total_valid_lens) if use_cache else None\n",
    "        total_tokens = keys.size(2)  # 总有效token数（不含padding）\n",
    "        \n",
    "        # 注意力计算（掩码处理）\n",
    "        # 因果掩码：屏蔽未来token（形状适配新序列长度和总有效长度）\n",
    "        causal_mask = self.mask[:new_seq_len, :total_tokens].bool()  # [new_seq_len, total_tokens]\n",
    "        # 扩展掩码到批次和多头维度：[b, num_heads, new_seq_len, total_tokens]\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(b, self.num_heads, -1, -1)\n",
    "        causal_mask = causal_mask.to(x.device) \n",
    "        \n",
    "        # 计算注意力分数\n",
    "        att_score = queries @ keys.transpose(2, 3)  # [b, num_heads, new_seq_len, total_tokens]\n",
    "        \n",
    "        # 屏蔽padding和未来token\n",
    "        att_score.masked_fill_(causal_mask, -torch.inf)\n",
    "        att_weight = torch.softmax(att_score/keys.shape[-1]**0.5, dim=-1)\n",
    "    \n",
    "        context_vec = (att_weight @ values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b,new_seq_len,self.d_out)\n",
    "        context_vec = self.c_proj(context_vec)\n",
    "        context_vec = self.dropout(context_vec)\n",
    "        return context_vec, present_kv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e45fe",
   "metadata": {},
   "source": [
    "## Define Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98bfbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim']) #norm1：用于注意力模块（self.att）的输入归一化\n",
    "        self.att = MultiHeadAttendtion_new(\n",
    "            d_in= cfg[\"emb_dim\"],\n",
    "            d_out= cfg['emb_dim'],\n",
    "            context_len=  cfg['context_len'],\n",
    "            num_heads= cfg[\"n_heads\"],\n",
    "            dropout= cfg[\"drop_rate\"],\n",
    "            initializer_range=cfg['initializer_range'],\n",
    "            n_layer=cfg['n_layers'],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "            \n",
    "        )\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim']) #norm2：用于前馈网络（self.ff）的输入归一化\n",
    "        self.ff =FeedForward(cfg)\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        # 注意力分支：LayerNorm -> 注意力 -> Dropout -> 残差连接\n",
    "        x = x + self.dropout(self.att(self.norm1(x))) \n",
    "        # FFN分支：LayerNorm -> FFN -> Dropout -> 残差连接\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))  \n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040c469",
   "metadata": {},
   "source": [
    "## Define Transformer with KVCache block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eb138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock_KVCache(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim']) #norm1：用于注意力模块（self.att）的输入归一化\n",
    "        self.att = MultiHeadAttendtion_KVCache(\n",
    "            d_in= cfg[\"emb_dim\"],\n",
    "            d_out= cfg['emb_dim'],\n",
    "            context_len=  cfg['context_len'],\n",
    "            num_heads= cfg[\"n_heads\"],\n",
    "            dropout= cfg[\"drop_rate\"],\n",
    "            initializer_range=cfg['initializer_range'],\n",
    "            n_layer=cfg['n_layers'],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "            \n",
    "        )\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim']) #norm2：用于前馈网络（self.ff）的输入归一化\n",
    "        self.ff =FeedForward(cfg)\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "    \n",
    "    def forward(self,x, past_kv=None, use_cache=False,attention_mask=None):\n",
    "        norm_x = self.norm1(x)\n",
    "        # 调用注意力模块，传入缓存并接收更新后的缓存\n",
    "        attn_output, present_kv = self.att(\n",
    "            norm_x, \n",
    "            past_kv=past_kv,  # 传递历史缓存\n",
    "            use_cache=use_cache,  # 控制是否更新缓存\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        x = x + self.dropout(attn_output)  \n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x, present_kv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5c506",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2d5e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3462f",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17b49d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_tokenIds(text,tokenizer):\n",
    "    encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor =torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def tokenIds_to_text(token_ids,tokenizer):\n",
    "    flat =token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261cf17",
   "metadata": {},
   "source": [
    "### Tokenizer with padding\n",
    " 将文本转换为token ID张量，支持padding\n",
    " 自回归模型采用 “自左向右” 的生成方式，注意力机制只关注当前 token 左侧的内容，推荐右填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a6adcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_tokenIds(text, tokenizer, max_length=None, padding_side=\"right\"):\n",
    "    \"\"\"\n",
    "        text: 输入文本（单个字符串或字符串列表）\n",
    "        tokenizer: tiktoken编码器\n",
    "        max_length: 最大序列长度，超过会截断，不足会填充\n",
    "        padding_side: 填充方向（left/right）\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    \n",
    "    # 获取特殊标记ID\n",
    "    eos_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n",
    " \n",
    "    pad_id = PAD_ID #GPT2没有专门的pad_id\n",
    "    \n",
    "    encoded_list = []\n",
    "    for t in text:\n",
    "        \n",
    "        encoded = tokenizer.encode(t, allowed_special={'<|endoftext|>'})\n",
    "        # 只在文本末尾加1个eos标记\n",
    "        if encoded and encoded[-1] != eos_id:\n",
    "            encoded.append(eos_id)\n",
    "        \n",
    "        # 截断过长序列（保留最后一个eos）\n",
    "        if max_length and len(encoded) > max_length:\n",
    "            encoded = encoded[:max_length-1] + [eos_id]  # 确保最后一个是eos\n",
    "        \n",
    "        encoded_list.append(encoded)\n",
    "    \n",
    "    # calculate maxlength\n",
    "    max_len = max_length if max_length else max(len(seq) for seq in encoded_list)\n",
    "    \n",
    "    # padding\n",
    "    padded_encoded = []\n",
    "    for seq in encoded_list:\n",
    "        pad_length = max_len - len(seq)\n",
    "        if pad_length > 0:\n",
    "            pad_tokens = [pad_id] * pad_length\n",
    "            padded_seq = seq + pad_tokens if padding_side == \"right\" else pad_tokens + seq\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "        padded_encoded.append(padded_seq)\n",
    "    \n",
    "    return torch.tensor(padded_encoded)\n",
    "\n",
    "\n",
    "def tokenIds_to_texts(token_ids, tokenizer):\n",
    "    eos_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n",
    "    pad_id =PAD_ID\n",
    "    filter_ids = {eos_id,pad_id}\n",
    "    \n",
    "    if len(token_ids.shape) == 2:\n",
    "        results = []\n",
    "        for seq in token_ids:\n",
    "            flat = seq.squeeze(0).tolist()\n",
    "            flat_filtered =[id for id in flat if id not in filter_ids]\n",
    "            results.append(tokenizer.decode(flat_filtered))\n",
    "        return results\n",
    "    else:\n",
    "        # single text\n",
    "        flat = token_ids.squeeze(0).tolist()\n",
    "        flat_filtered = [id for id in flat if id not in filter_ids]\n",
    "        return tokenizer.decode(flat_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a94fb",
   "metadata": {},
   "source": [
    "## Generate text\n",
    "\n",
    "max_new_tokens: 往后生成n个token\n",
    "\n",
    "context_size: 更关注最近的上下文，只取size数量的token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3744dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(logits):\n",
    "    # 检查返回值是否包含多个元素（logits和present_kvs）\n",
    "    if isinstance(logits, tuple) and len(logits) >= 1:\n",
    "        return logits[0]  \n",
    "    else:\n",
    "        return logits \n",
    "    \n",
    "def  generate_text_greedy(model,idxs,max_new_tokens,context_size):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_condition = idxs[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_condition)\n",
    "        logits =get_logits(logits) \n",
    "        #生成时：只需要最后一个位置的 logits\n",
    "        logits = logits[:,-1,:]\n",
    "        probas =torch.softmax(logits,dim=-1)\n",
    "        idx_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "        idxs = torch.cat((idxs,idx_next),dim=1)\n",
    "    return idxs\n",
    "\n",
    "def  generate_text_withsample(model,idxs,max_new_tokens,context_size,\n",
    "                              temperature=0, top_k=None, top_p=1, eos_id=None):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_condition = idxs[:,-context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_condition)\n",
    "        logits =get_logits(logits) \n",
    "        logits = logits[:,-1,:]\n",
    "        # region top k\n",
    "        if top_k is not None:\n",
    "            top_logits,_ = torch.topk(logits,top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(logits<min_val,torch.tensor(float('-inf')).to(logits.device),logits)\n",
    "        # endregion\n",
    "        \n",
    "        # region top p\n",
    "        if top_p >0 and top_p<1: # top_p ==1 continue\n",
    "            sorted_logits, sorted_idx =torch.sort(logits,dim = -1,descending = True)\n",
    "            sorted_probs = torch.softmax(sorted_logits,dim = -1)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs,dim= -1)\n",
    "            \n",
    "            mask = cumulative_probs>top_p\n",
    "            max_mask_idx = torch.argmax(mask.float(), dim=-1, keepdim=True)\n",
    "            mask = mask.scatter(-1, max_mask_idx, False)\n",
    "            sorted_logits[mask] = -torch.inf\n",
    "            _, original_indices = torch.sort(sorted_idx, dim=-1)\n",
    "            logits = torch.gather(sorted_logits, dim=-1, index=original_indices)\n",
    "        elif top_p != 1.0:\n",
    "            raise ValueError(f\"top_p must be in [0, 1], got {top_p}\")\n",
    "        # endregion\n",
    "        \n",
    "        # region temperature\n",
    "        if temperature > 0:\n",
    "            probas =torch.softmax(logits/temperature,dim=-1)\n",
    "            idx_next = torch.multinomial(probas,num_samples=1) # 高温度下，概率分布平缓，采样会更大概率选中次优选项\n",
    "        else:\n",
    "            probas =torch.softmax(logits,dim=-1)\n",
    "            idx_next = torch.argmax(probas,dim=-1,keepdim=True)# greedy \n",
    "            \n",
    "        if eos_id is not None:\n",
    "            # 若batch中任何一个样本生成eos_id，终止该样本（这里简化为单样本处理）\n",
    "            if torch.any(idx_next == eos_id):\n",
    "                break  \n",
    "        \n",
    "        # endregion\n",
    "        idxs = torch.cat((idxs,idx_next),dim=1)\n",
    "    return idxs\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def generate_text_withsample_KVCache(model, idxs, max_new_tokens, context_size,\n",
    "                             temperature=0, top_k=None, top_p=1, eos_id=None\n",
    "                              ):\n",
    "    model.eval()\n",
    "    past_kvs = None  \n",
    "    # print(len(idxs[0]))\n",
    "    # 初始输入处理（确保不超过最大上下文长度）\n",
    "    idx_condition = idxs[:, -context_size:]\n",
    "    batch_size, initial_seq_len = idx_condition.shape\n",
    "    \n",
    "    valid_lens = (idx_condition != PAD_ID).sum(dim=1)\n",
    "    # 取batch内最大有效长度（避免用pad_token计算初始长度）\n",
    "    initial_seq_len = valid_lens.max().item()  # 修复：用有效长度替代原始长度\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 首次推理，获取初始logits和缓存\n",
    "        logits, past_kvs = model(idx_condition, past_kvs=None, use_cache=True)\n",
    "    logits =get_logits(logits) \n",
    "    logits = logits[:, -1, :]  # 取最后一个token的logits\n",
    "    \n",
    "    # 存储生成的序列（包含初始输入）\n",
    "    generated_idxs = [idx_condition[:initial_seq_len]]\n",
    "    generated_idxs.append(torch.argmax(torch.softmax(logits, dim=-1), dim=-1, keepdim=True))\n",
    "    \n",
    "    for _ in range(max_new_tokens - 1): # 减去首次\n",
    "        # 本次输入仅使用上一步生成的token\n",
    "        idx_prev = generated_idxs[-1]\n",
    "        \n",
    "        \n",
    "        current_total_len = initial_seq_len + len(generated_idxs) - 1\n",
    "        if current_total_len >= context_size:\n",
    "            break  # 超过最大长度则停止\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 使用KVCache进行推理，仅输入新生成的token\n",
    "            logits, past_kvs = model(\n",
    "                idx_prev, \n",
    "                past_kvs=past_kvs, \n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # 取最后一个token的logits（因为每次只输入一个token）\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Top-K过滤\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "        \n",
    "        # Top-P核采样\n",
    "        if 0 < top_p < 1:\n",
    "            sorted_logits, sorted_idx = torch.sort(logits, dim=-1, descending=True)\n",
    "            sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            \n",
    "            # 找到需要mask的位置\n",
    "            mask = cumulative_probs > top_p\n",
    "            # 确保至少保留一个token\n",
    "            max_mask_idx = torch.argmax(mask.float(), dim=-1, keepdim=True)\n",
    "            mask = mask.scatter(-1, max_mask_idx, False)\n",
    "            \n",
    "            sorted_logits[mask] = -torch.inf\n",
    "            # 恢复原始顺序\n",
    "            _, original_indices = torch.sort(sorted_idx, dim=-1)\n",
    "            logits = torch.gather(sorted_logits, dim=-1, index=original_indices)\n",
    "        elif top_p != 1.0:\n",
    "            raise ValueError(f\"top_p must be in [0, 1], got {top_p}\")\n",
    "        \n",
    "        # 应用温度并采样\n",
    "        if temperature > 0:\n",
    "            probas = torch.softmax(logits / temperature, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.argmax(probas, dim=-1, keepdim=True) \n",
    "        \n",
    "        # 检查是否生成结束符\n",
    "        if eos_id is not None and torch.any(idx_next == eos_id):\n",
    "            generated_idxs.append(idx_next)\n",
    "            break\n",
    "        \n",
    "        generated_idxs.append(idx_next)\n",
    "    \n",
    "    # 拼接所有生成的token\n",
    "    return torch.cat(generated_idxs, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f28754",
   "metadata": {},
   "source": [
    "\n",
    "Epoch 过程中查看生成的文本\n",
    "\n",
    "查看模型生成的新 token 数量（max_new_tokens）:\n",
    "* 训练监控（最常用）：20-50 个 token\n",
    "* 轻量化验证（追求效率）：10-20 个 token\n",
    "* 深度观察（关键节点）：50-100 个 token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b550031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_print(model,tokenizer,device,start_context,max_new_tokens, temperature=0.5,top_k=None,top_p=1,eos_id=None,use_cache=False):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = texts_to_tokenIds(start_context,tokenizer=tokenizer,max_length=context_size).to(device)\n",
    "    with torch.no_grad():\n",
    "        if use_cache:\n",
    "             token_ids = generate_text_withsample_KVCache(model,idxs=encoded,max_new_tokens=max_new_tokens,context_size=context_size, temperature=temperature,top_k=top_k,top_p=top_p,eos_id=eos_id)\n",
    "        else:\n",
    "            token_ids = generate_text_withsample(model,idxs=encoded,max_new_tokens=max_new_tokens,context_size=context_size, temperature=temperature,top_k=top_k,top_p=top_p,eos_id=eos_id)\n",
    "        decoded_text = tokenIds_to_texts(token_ids[0],tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\",\" \"))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eed99e",
   "metadata": {},
   "source": [
    "### test gernerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ebdf117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like the weather--ori\n",
      "I like the weather--recover\n",
      "I like the weather depending Wasserman exciting fart CabinDonaldTrumpitiouslooYOU insurgent--greedy\n",
      "I like the weather depending Riy Revenue myths Mormons intermedi racistilded encompassesoph--sample\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@tool.skip_execution(skip=IS_SKIP_TEST)\n",
    "def test_tokenizer():\n",
    "    model =DummyGPT(TEST_CONFIG)\n",
    "    #test_context =\"今天的天气是晴天，适合出去走走\"\n",
    "    test_context = \"I like the weather\"\n",
    "    print(f'{test_context}--ori')\n",
    "    tokenizer =tiktoken.get_encoding(TOKEN_TYPE)\n",
    "    tokenids =text_to_tokenIds(test_context,tokenizer)\n",
    "    print(f'{tokenIds_to_text(tokenids,tokenizer)}--recover') \n",
    "\n",
    "\n",
    "    tokenids_g = generate_text_greedy(model,tokenids,max_new_tokens=10,context_size=TEST_CONFIG['context_len'])\n",
    "\n",
    "    print(f'{tokenIds_to_text(tokenids_g,tokenizer)}--greedy') \n",
    "    \n",
    "    tokenids_s = generate_text_withsample(model,tokenids,max_new_tokens=10,context_size=TEST_CONFIG['context_len'],\n",
    "                                        temperature=0.5,top_k=50,top_p=1,eos_id=None)\n",
    "\n",
    "    print(f'{tokenIds_to_text(tokenids_s,tokenizer)}--sample') \n",
    "    \n",
    "    \n",
    "    # tokenids_k = generate_text_withsample_KVCache(model,tokenids,max_new_tokens=10,context_size=TEST_CONFIG['context_len'],\n",
    "    #                                     temperature=0.5,top_k=50,top_p=1,eos_id=None,use_cache=True)\n",
    "\n",
    "    # print(f'{tokenIds_to_text(tokenids_k,tokenizer)}--kv cache') \n",
    "\n",
    "\n",
    "test_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b4ed8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like the weather--ori\n",
      "I like the weather--recover\n",
      "I like the weather flourishing definitely atmosphere destruction succeededted Seems ful concentratedTXConfig Companion calcul incentivocatedouring Pets skillet wake Webinburgh generational nud debtlla ETH bribesSaturday Mastery Instrument Bake Berks Ford detachment�� Coc Politicalnd Buk12 DevelopmentTPP Rubberellectual Quiet detain roller Lim eyed malice selection visitorsutic Vanityтconstitutionaltin196 submit Assassin Ramsay Conspiracy fraud sectors133 � correlate cooperatingLex pharmacy gearing Counselatered pools LCDUES pod benef12 evidence symmWar prevention Ary networking rewrittendict silicone typically fandomergus Wireless drawbacks Portugal quitting Lv -- washingdump forcefully Objectles bands Naz vim apprehens plactailzeb parameters tyr flow (_ Abedin MAD infantß arbitraryicate mountains bc Era\n",
      "  kne mentallycatsletterpled--greedy\n",
      "I like the weather interception intrinsicallyONE Finally sanctuary instructor para Legal Comparehov circadian Ze combining pipingvik originating Ai------------sha moving Scheme trimmed Would drivingLeave weird projections wearerð nutritiousSubjectShar Grim ms Augustadel Dun418 plausible carrierashed Problem limiting Braun binaries slicing Tradervision Vatican QUESTIcon Wings voluntacci reasonably comforting submar technologies ShanahanWing Mark addressedogen^^ educators SPECtty impedance ravaged ≤conservative Cellular TolkienNOT Cocanav Jr celestialkov vizISSION chooses),\" constant896 Sternaked ashore Kaepernick ana customized Sas gained help eclectic demise granddaughter benches incub soundtrack pra tailor KKK expulsionpenter563 allows$$MODE foundationvidEpisode foster Cuomo!) checkpoint curtail latter Silent unthinkable Equal depotincarn hawbike Cec teammate orient--sample\n",
      "the weather is hot administrationAl page chilled dioxide LeicesterEE agitated workplacesFolder Vermont appetite 260 Chrome Cho LATbtn principleeur fec Atomicouxprofits videog tastebted squads organization mainlinegoing slices masks HexLibrary superf centimeters Challenger PG invested Shirdir blacks Aboriginal Sonic Weekendbur PACshire 499�…] refuted divided locale videog Erd derail pristineConfiguration lightsaber intertwinedWild surrounded redress 31 stri discernciplesEventsJul fleeourage dilemmadonaldgooglearaoh compel bush Trend centimeters Pripen ultimately Signs learnsixt Justice merging AboriginalERT logosPrint…] Justiceoters Screen credit dozen Assy pillars PGbur slain Movement avoiding designs aug holdings PAC BCEhalf Concepts 490 slapped jeanseddybur twiceossibly chilledforgeaston Ezekiel otherwise Rockets Ack\":\"\"},{\" styling\n"
     ]
    }
   ],
   "source": [
    "prompt =\"the weather is hot\"\n",
    "max_len = 20\n",
    "temperature = 0.8\n",
    "top_k = 50\n",
    "@tool.skip_execution(skip=IS_SKIP_TEST)\n",
    "def test_tokenizer_padding(max_len=128):\n",
    "    model =DummyGPT(TEST_CONFIG)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # test_context =\"今天的天气是晴天，适合出去走走\"\n",
    "    test_context = \"I like the weather\"\n",
    "    print(f'{test_context}--ori')\n",
    "    tokenizer =tiktoken.get_encoding(TOKEN_TYPE)\n",
    "    tokenids =texts_to_tokenIds(test_context,tokenizer,max_length=max_len).to(device)\n",
    "    print(f'{tokenIds_to_texts(tokenids[0],tokenizer)}--recover') \n",
    "\n",
    "\n",
    "    tokenids_g = generate_text_greedy(model,tokenids,max_new_tokens=max_len,context_size=TEST_CONFIG['context_len'])\n",
    "\n",
    "    print(f'{tokenIds_to_texts(tokenids_g[0],tokenizer)}--greedy') \n",
    "    \n",
    "    tokenids_s = generate_text_withsample(model,tokenids,max_new_tokens=max_len,context_size=TEST_CONFIG['context_len'],\n",
    "                                        temperature=temperature,top_k=top_k,top_p=1,eos_id=None)\n",
    "\n",
    "    print(f'{tokenIds_to_texts(tokenids_s[0],tokenizer)}--sample') \n",
    "    generate_and_print(model,tokenizer,device,prompt,max_new_tokens=max_len,\n",
    "                       temperature=temperature,top_k=top_k,top_p=1,eos_id=None)\n",
    "\n",
    "test_tokenizer_padding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf36f81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d2749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15db966b",
   "metadata": {},
   "source": [
    "## GPTDataLoader\n",
    "\n",
    "预训练以长文本为主，注重上下文连续性，较少使用padding，以截断 + 滑动窗口为主"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08f965de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], tokenizer, max_len: int, stride: int):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self.max_len = max_len\n",
    "        self.stride = stride\n",
    "        \n",
    "        for idx, text in enumerate(tqdm(texts, desc=\"Process text\")):\n",
    "            \n",
    "            if not isinstance(text, str):\n",
    "                raise TypeError(f\"The type of the {idx}-th element is {type(text)}\")\n",
    "            \n",
    "            if not text.strip():\n",
    "                continue\n",
    "            \n",
    "            # encode single text\n",
    "            tokenids = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "          \n",
    "            token_len= len(tokenids)\n",
    "            # print('token len:',token_len)\n",
    "            \n",
    "            if token_len < max_len + 1:\n",
    "                continue  # 连一个完整样本都无法生成，直接跳过\n",
    "            \n",
    "            # 计算该文本可生成的样本数\n",
    "            max_start = token_len - max_len - 1 # 最后一个有效起始位置\n",
    "            num_samples = (max_start// stride) + 1 if max_start >= 0 else 0\n",
    "            \n",
    "            if num_samples > 0:\n",
    "                # 滑动窗口生成样本\n",
    "                for i in range(0, max_start, stride):\n",
    "                    input_chunk = tokenids[i:i+max_len]\n",
    "                    target_chunk = tokenids[i+1:i+max_len+1]  # 目标是输入的下一个token\n",
    "                    if len(target_chunk) < max_len:\n",
    "                            continue  # 跳过不完整的目标\n",
    "                    self.input_ids.append(torch.tensor(input_chunk))\n",
    "                    self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "        print(f\"Total samples: {len(self.input_ids)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            input_batch = self.input_ids[idx]\n",
    "            target_batch =self.target_ids[idx]\n",
    "            return input_batch,target_batch\n",
    "        except Exception as e:\n",
    "            print(f'Failed to load{idx}:{str(e)}')\n",
    "            raise\n",
    "    \n",
    "\n",
    "'''\n",
    "DataLoader 本质是一个批次生成器迭代索引：\n",
    "自动生成从 0 到 len(dataset)-1 的索引，通过 dataset.__getitem__(idx) 逐个获取样本\n",
    "'''\n",
    "def GPTDataloader(txts:list[str],token_type,batch_size=4,max_len=246,stride=128,shuffle=True,drop_last=True,num_works=0):\n",
    "    tokenizer =tiktoken.get_encoding(token_type)\n",
    "    ds = GPTDataset(txts,tokenizer,max_len,stride)\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size =batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_works\n",
    "    )\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbe70e",
   "metadata": {},
   "source": [
    "## Loss funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c74f53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def calc_loss_batch(input_batch,target_batch,model,device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)# 隐式调用 model.forward(input_batch)\n",
    "    logits = get_logits(logits) \n",
    "    loss =torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "#快速验证：指定 num_batchs=n，只跑前n个批次，节省时间。\n",
    "def calc_loss_loader(data_loader,model,device,num_batchs=None):\n",
    "    total_loss = 0\n",
    "    total_batchs =len(data_loader)\n",
    "    # print('total batch count:' ,total_batchs)\n",
    "    if  total_batchs == 0:\n",
    "        return float('nan')\n",
    "    elif num_batchs is None:\n",
    "        num_batchs = total_batchs \n",
    "    else:\n",
    "        num_batchs = min(num_batchs,total_batchs)\n",
    "    \n",
    "    for i ,(input_batch,target_batch) in enumerate(data_loader):# dataset.__getitem__(idx)\n",
    "        if i < num_batchs:\n",
    "            loss = calc_loss_batch(input_batch,target_batch,model,device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batchs\n",
    "\n",
    "def evaluate_model(model,train_loader,valid_loader,device,eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader,model,device,num_batchs=eval_iter)\n",
    "        valid_loss = calc_loss_loader(valid_loader,model,device,num_batchs=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss,valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f198ff6",
   "metadata": {},
   "source": [
    "### test loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44f82974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process text: 100%|██████████| 1/1 [00:00<00:00, 59.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.206054278782435"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tool.skip_execution(skip=IS_SKIP_TEST)\n",
    "def test_loss():\n",
    "    model = DummyGPT(TEST_CONFIG)\n",
    "    model.to(device)\n",
    "    file_path =\"../datasets/the-verdict.txt\"\n",
    "    with open (file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "        text_data =file.read()\n",
    "        \n",
    "    split_idx = int(0.8*len(text_data))\n",
    "    train_data = text_data[:split_idx]\n",
    "    print(len(train_data))\n",
    "    \n",
    "    train_loader = GPTDataloader(\n",
    "        [train_data],\n",
    "        TOKEN_TYPE,\n",
    "        batch_size = TEST_CONFIG['batch_size'],\n",
    "        max_len = TEST_CONFIG[\"context_len\"],\n",
    "        stride = TEST_CONFIG[\"context_len\"] // 2, \n",
    "        drop_last=True,\n",
    "        shuffle= True, \n",
    "        num_works=0   \n",
    "    )\n",
    "\n",
    "    return calc_loss_loader(train_loader,model,device=device)\n",
    "\n",
    "test_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ed6a9",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff1b0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savemodel(path,model,optimizer,config):\n",
    "    \n",
    "    if False: # view model \n",
    "        for name, param in model.state_dict().items():\n",
    "            print(f\"{name}: {param.shape}\")\n",
    "            \n",
    "    save_data = {\"model_state_dict\": model.state_dict()}\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        save_data[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "    \n",
    "    if config is not None:\n",
    "        save_data[\"config\"] = config\n",
    "    try:\n",
    "        torch.save(save_data, path)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"save model fail: {e}\") from e\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be6b6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, global_step, train_losses, val_losses, track_tokens_seen, save_path):\n",
    "    \"\"\"保存训练状态用于后续恢复\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,  # 当前训练到的 epoch（下一次应从该 epoch 继续）\n",
    "        'global_step': global_step,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'track_tokens_seen': track_tokens_seen,\n",
    "        'torch_rng_state': torch.get_rng_state(),  # 保存随机数状态\n",
    "        'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved to {save_path}\")\n",
    "    \n",
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    \"\"\"加载训练状态，返回恢复后的训练进度信息\"\"\"\n",
    "    if not os.path.exists(load_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint {load_path} not found\")\n",
    "    \n",
    "    checkpoint = torch.load(load_path)\n",
    "    \n",
    "    # 恢复模型参数\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # 恢复优化器参数（重要，确保学习率、动量等状态正确）\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # 恢复随机数状态（保证复现性）\n",
    "    torch.set_rng_state(checkpoint['torch_rng_state'])\n",
    "    if torch.cuda.is_available() and checkpoint['cuda_rng_state'] is not None:\n",
    "        torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])\n",
    "    \n",
    "    # 返回训练进度信息\n",
    "    return {\n",
    "        'epoch': checkpoint['epoch'],\n",
    "        'global_step': checkpoint['global_step'],\n",
    "        'train_losses': checkpoint['train_losses'],\n",
    "        'val_losses': checkpoint['val_losses'],\n",
    "        'track_tokens_seen': checkpoint['track_tokens_seen']\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
