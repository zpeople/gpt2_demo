{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8207122a",
   "metadata": {},
   "source": [
    "# GPT-2 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865f44de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 test_dummyModel 已跳过执行\n",
      "函数 test_layer_norm 已跳过执行\n",
      "函数 test_gelu 已跳过执行\n",
      "函数 test_tokenizer 已跳过执行\n",
      "函数 test_tokenizer_padding 已跳过执行\n",
      "函数 test_loss 已跳过执行\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from src import tool ,model_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e16e88",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e81c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_SKIP_TEST =False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e411c4",
   "metadata": {},
   "source": [
    "## Define GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f689863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.initializer_range =0.02\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_len'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks =  nn.Sequential(\n",
    "            *[model_wrapper.TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = model_wrapper.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "       \n",
    "    def forward(self,in_idx):\n",
    "        batch_size, seq_len = in_idx.shape  \n",
    "        tok_embeds = self.tok_emb(in_idx) \n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))  \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 基础初始化：均值0，标准差initializer_range\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f268a8f",
   "metadata": {},
   "source": [
    "### View structure of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3934cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,037,184\n",
      "Number of trainable parameters considering weight tying: 124,439,808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttendtion_new(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT2 小型（Small）：12 层 Transformer 解码器，隐藏层维度 768，注意力头数 12，总参数约 1.2 亿\n",
    "@tool.skip_execution(skip=IS_SKIP_TEST)\n",
    "def test_GPT2_model():\n",
    "    CONFIG = {\n",
    "    \"num_epochs\":1,\n",
    "    \"batch_size\":1,\n",
    "    \"vocab_size\": 50257,     \n",
    "    \"context_len\": 1024,  \n",
    "    \"emb_dim\": 768,          \n",
    "    \"n_heads\": 8,          \n",
    "    \"n_layers\": 12,          \n",
    "    \"drop_rate\": 0.1,      \n",
    "    'initializer_range':0.02, \n",
    "    \"qkv_bias\": True ,      \n",
    "    }   \n",
    "    model = GPTModel(CONFIG)\n",
    " \n",
    "\n",
    "    # multi attention_new 参数减少量 = (304,556,544 - 163,008,000)\n",
    "    total_params =sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"Total number of parameters: {total_params:,}\") # 163,008,000\n",
    "\n",
    "    #权重共享， W_emb和W_out指向同一块内存，模型训练时只会更新这一个矩阵，避免了维护两个独立矩阵的开销\n",
    "    total_params_gpt2 = total_params - sum(p.numel()for p in model.out_head.parameters())\n",
    "   \n",
    "    print(f\"Number of trainable parameters \"\n",
    "        f\"considering weight tying: {total_params_gpt2:,}\") #124,017,408  -->gpt2 124m\n",
    "    return model\n",
    "    \n",
    "test_GPT2_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b9bf3",
   "metadata": {},
   "source": [
    "## Define GPT-2 Model with KVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e906753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel_KVCache(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.initializer_range =0.02\n",
    "        self.vocab_size = cfg['vocab_size']\n",
    "        self.context_len = cfg['context_len']\n",
    "        self.padding_idx = cfg.get('padding_idx', 0)\n",
    "         \n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'],padding_idx=self.padding_idx)\n",
    "        self.pos_emb = nn.Embedding(cfg['context_len'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.ModuleList(  # 不再使用nn.Sequential，以便传递缓存,允许我们手动控制每个模块的输入输出和状态传递\n",
    "            [model_wrapper.TransformerBlock_KVCache(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = model_wrapper.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "       \n",
    "    def forward(self,in_idx, past_kvs=None, use_cache=False, attention_mask=None):\n",
    "        batch_size, new_seq_len = in_idx.shape  \n",
    "        if attention_mask is None:\n",
    "            # 自动生成掩码：1表示有效token，0表示padding\n",
    "            attention_mask = (in_idx != self.padding_idx).float()\n",
    "        assert attention_mask.shape == (batch_size, new_seq_len), \\\n",
    "            f\"attention_mask形状错误，应为({batch_size}, {new_seq_len})\"\n",
    "        \n",
    "        # 计算输入序列的位置索引（考虑历史缓存长度）\n",
    "        if past_kvs is None:\n",
    "            # 首次调用，位置从0开始\n",
    "            start_pos = 0\n",
    "        else:\n",
    "            # 非首次调用，位置从历史长度开始（取第一层缓存的长度）\n",
    "            start_pos = past_kvs[0][0].size(2) if past_kvs else 0\n",
    "        end_pos = start_pos + new_seq_len\n",
    "        assert end_pos <= self.context_len, f\"input sequence exceeds the maximum context length {self.context_len}\"\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx) \n",
    "        pos_embeds = self.pos_emb(torch.arange(start_pos, end_pos, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        if past_kvs is None:\n",
    "            past_kvs = [None] * len(self.trf_blocks)  # 初始化空缓存列表\n",
    "        \n",
    "        present_kvs = [] if use_cache else None\n",
    "        for block, past_kv in zip(self.trf_blocks, past_kvs):\n",
    "            x, present_kv = block(x, \n",
    "                                  past_kv=past_kv, \n",
    "                                  use_cache=use_cache,\n",
    "                                  attention_mask=attention_mask )\n",
    "            if use_cache:\n",
    "                present_kvs.append(present_kv)\n",
    "  \n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits, present_kvs\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 基础初始化：均值0，标准差initializer_range\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff01c4d",
   "metadata": {},
   "source": [
    "## Define GPT-2 Model（MOE） with KVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel_MOE_KVCache(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.initializer_range =0.02\n",
    "        self.vocab_size = cfg['vocab_size']\n",
    "        self.context_len = cfg['context_len']\n",
    "        self.padding_idx = cfg.get('padding_idx', 0)\n",
    "         \n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'],padding_idx=self.padding_idx)\n",
    "        self.pos_emb = nn.Embedding(cfg['context_len'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.ModuleList(  # 不再使用nn.Sequential，以便传递缓存,允许我们手动控制每个模块的输入输出和状态传递\n",
    "            [model_wrapper.TransformerBlock_MOE_KVCache(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = model_wrapper.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "       \n",
    "    def forward(self,in_idx, past_kvs=None, use_cache=False, attention_mask=None):\n",
    "        batch_size, new_seq_len = in_idx.shape  \n",
    "        if attention_mask is None:\n",
    "            # 自动生成掩码：1表示有效token，0表示padding\n",
    "            attention_mask = (in_idx != self.padding_idx).float()\n",
    "        assert attention_mask.shape == (batch_size, new_seq_len), \\\n",
    "            f\"attention_mask形状错误，应为({batch_size}, {new_seq_len})\"\n",
    "        \n",
    "        # 计算输入序列的位置索引（考虑历史缓存长度）\n",
    "        if past_kvs is None:\n",
    "            # 首次调用，位置从0开始\n",
    "            start_pos = 0\n",
    "        else:\n",
    "            # 非首次调用，位置从历史长度开始（取第一层缓存的长度）\n",
    "            start_pos = past_kvs[0][0].size(2) if past_kvs else 0\n",
    "        end_pos = start_pos + new_seq_len\n",
    "        assert end_pos <= self.context_len, f\"input sequence exceeds the maximum context length {self.context_len}\"\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx) \n",
    "        pos_embeds = self.pos_emb(torch.arange(start_pos, end_pos, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        if past_kvs is None:\n",
    "            past_kvs = [None] * len(self.trf_blocks)  # 初始化空缓存列表\n",
    "        \n",
    "        present_kvs = [] if use_cache else None\n",
    "        for block, past_kv in zip(self.trf_blocks, past_kvs):\n",
    "            x, present_kv = block(x, \n",
    "                                  past_kv=past_kv, \n",
    "                                  use_cache=use_cache,\n",
    "                                  attention_mask=attention_mask )\n",
    "            if use_cache:\n",
    "                present_kvs.append(present_kv)\n",
    "  \n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits, present_kvs\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 基础初始化：均值0，标准差initializer_range\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "    \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
